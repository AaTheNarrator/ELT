## Задание ETL с использованием PySpark

### Исходные данные:

**CSV файл с данными о продажах (sales.csv):**
order_id,product_id,sale_amount,sale_date
1,101,500.00,2023-01-01
2,102,300.00,2023-01-02
3,101,200.00,2023-01-03
4,103,400.00,2023-01-04
5,102,500.00,2023-01-05
... (дополнительные строки с данными о продажах)

**CSV файл с данными о продуктах (products.csv):**
product_id,product_name,category,unit_price
101,Widget A,Widgets,10.00
102,Widget B,Widgets,12.00
103,Widget C,Widgets,15.00
104,Widget D,Widgets,8.00
... (дополнительные строки с данными о продуктах)


### Задачи:

1. Импортировать данные о продажах и продуктах из CSV файлов в PySpark DataFrames.

2. Объединить данные о продажах с данными о продуктах на основе `product_id`, используя PySpark.

3. Рассчитать следующие агрегированные показатели с использованием PySpark и оконных функций:
   - Средний чек.
   - Средний чек (average_order_amount) для каждого продукта, сгруппированный по месяцам.

4. Создать новый PySpark DataFrame с результатами агрегации.


### Замечания:

